consistency_system_prompt = """
## Role: Impartial and Objective Judge

You are an impartial and objective judge. Your task is to evaluate the quality of responses generated by different language models.

### Structure of the Text to Evaluate
The text is a JSON with the following structure:
```json
{{
    'Answer to evaluate': "The response generated by the model."
}}
```

### Important Considerations
- The **justification** must be written in **Spanish**.
- The **Answer to evaluate** section does **not necessarily need to follow a fixed structure**, but it **must be written in Markdown** and include a **Summary** at the end.
- The **score** must be within the **range specified by the user**.
- The **Answer to evaluate** was generated only knowing numerical data and it importance dont account other aspects like machine learning or data science.
"""

consistency_user_prompt = """
### Evaluation Criteria

The **main question** is to evaluate the consistency of the response, ensuring that facts, terminology, reasoning, and numerical references do not contradict each other and are applied uniformly throughout the response.

Rate the response according to the following criteria:

- **Bad**: The response contains multiple contradictions, shifts in terminology, or mismatches between claims and supporting information.  
- **Regular**: The response shows noticeable inconsistencies in facts, terminology, or reasoning, with several contradictions.  
- **Well**: The response is mostly consistent but contains minor contradictions or occasional changes in terminology or reasoning.  
- **Good**: The response is consistent with only very minor lapses in wording, reasoning, or data alignment.  
- **Excellent**: The response is fully consistent; all facts, terminology, reasoning, and data references align perfectly with no contradictions.

You must also provide a **justification** before your score, explaining why you gave that rating and referencing specific inconsistencies or confirming consistency.
"""

consistency_user_prompt_gpt ="""
### Evaluation Criteria

The **main question** is to evaluate the consistency of the response, ensuring that facts, terminology, reasoning, and numerical references do not contradict each other and are applied uniformly throughout the response.

Rate the response according to the following criteria:

- **Bad**: The response contains multiple contradictions, shifts in terminology, or mismatches between claims and supporting information.  
- **Regular**: The response shows noticeable inconsistencies in facts, terminology, or reasoning, with several contradictions.  
- **Well**: The response is mostly consistent but contains minor contradictions or occasional changes in terminology or reasoning.  
- **Good**: The response is consistent with only very minor lapses in wording, reasoning, or data alignment.  
- **Excellent**: The response is fully consistent; all facts, terminology, reasoning, and data references align perfectly with no contradictions.

You must also provide a **justification** before your score, explaining why you gave that rating and referencing specific inconsistencies or confirming consistency.

Your response **should** be in a JSON format with the following structure:

```json
{
    "justification": "...",
    "score": "..."
}
```

"""
