{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLm as a Judge\n",
    "\n",
    "Para evaluar la calidad de las repuestas generadas por una llm se pueden usar diferentes metricas basandose en la opinión de un modelo externo, así se evaluaran diferentes criteterios como calidad de respuesta, relevancia."
   ],
   "id": "dc3d7323cc93cbb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:05.666948Z",
     "start_time": "2025-08-09T22:36:49.211821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from Levenshtein import ratio as levenshtein_ratio\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ],
   "id": "7d49cac799520bf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\migue\\anaconda3\\envs\\viu-AI\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:05.697194Z",
     "start_time": "2025-08-09T22:37:05.679977Z"
    }
   },
   "source": [
    "class BaseScorer:\n",
    "    def __init__(self, model, system_prompt:str, metric_name:str=None, structure=None, user_prompt:str=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.metric_name = metric_name\n",
    "        self.structure = structure\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = user_prompt\n",
    "        self.scores = []\n",
    "\n",
    "    def score(self, responses: list, questions: list) -> list:\n",
    "        model = ChatOllama(model=self.model, temperature=0.0)\n",
    "        structure_model = model.with_structured_output(self.structure)\n",
    "\n",
    "        eval_responses = []\n",
    "\n",
    "        for response, question in zip(responses , questions):\n",
    "            message = [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"# Instructions:\\n\\n{self.user_prompt}\\n\\n {{'Answer to evaluate' : {response}}}\"}\n",
    "                ]\n",
    "\n",
    "            score = structure_model.invoke(message)\n",
    "            eval_responses.append(score)\n",
    "\n",
    "        self.scores = eval_responses\n",
    "\n",
    "        return self.scores\n",
    "\n",
    "    def log_metrics(self, *args):\n",
    "        NotImplementedError(\"Este método es propio de la subclase, ya que dependerá de la información de las metricas\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Selección de juez\n",
    "\n",
    "Un juez debe ser imparcial, objetivo y no tener sesgos, por eso se debe evaluar los modelos disponibles y observar el que tenga menos varianza en sus respuestas. Para ellos se usa, tanto la varianza de las respuestas numericas como la diferencia entres sus justificaciones mediante embeddings."
   ],
   "id": "fd90eb483ca6aa52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:07.775940Z",
     "start_time": "2025-08-09T22:37:05.702709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "available_models = [\n",
    "    \"gpt-oss:20b\",\n",
    "    # \"llama3.1:8b\",\n",
    "    # \"mistral:latest\",\n",
    "    # \"gemma3:27b\",\n",
    "    # \"phi3:14b\",\n",
    "    # \"qwen3:14b\",\n",
    "    # \"deepseek-r1:32b\",\n",
    "]\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ],
   "id": "aced7e537d75660c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:08.100630Z",
     "start_time": "2025-08-09T22:37:08.075098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class JudgeOutput(BaseModel):\n",
    "    score: str\n",
    "    justification: str\n",
    "\n",
    "select_judge_system_prompt =\"\"\"\n",
    "Eres un juez imparcial y objetivo. Tu tarea es evaluar la calidad de las respuestas generadas por diferentes modelos de lenguaje.\n",
    "\n",
    "# Estructura del texto a evaluar:\n",
    "-  Pregunta: pregunta realizada por el usuario.\n",
    "-  Respuesta: respuesta generada por el modelo.\n",
    "\n",
    "# Considereaciones importantes:\n",
    "- La justificación debe estar en español.\n",
    "- El apartado Respuesta a evaluar no necesariamente debe seguir una estructura fija. Más sin embargo debe tener estar en Markdown y contener un Resumen al final.\n",
    "- Las calificación debe ser en el rango que especifique el usuario.\n",
    "\"\"\"\n",
    "select_judge_system_prompt = \"\"\"\n",
    "## Role: Impartial and Objective Judge\n",
    "\n",
    "You are an impartial and objective judge. Your task is to evaluate the quality of responses generated by different language models.\n",
    "\n",
    "### Structure of the Text to Evaluate\n",
    "The text is a JSON with the following structure:\n",
    "```json\n",
    "{{\n",
    "    \"Answer to evaluate\": \"The response generated by the model.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "### Important Considerations\n",
    "- The **justification** must be written in **Spanish**.\n",
    "- The **Answer** section does **not necessarily need to follow a fixed structure**, but it **must be written in Markdown** and include a **Summary** at the end.\n",
    "- The **rating** must be within the **range specified by the user**.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "### Evaluation Criteria\n",
    "\n",
    "Rate the response according to the following criteria:\n",
    "\n",
    "- **Bad**: The response is very poor and does not answer the question.\n",
    "- **Regular**: The response is poor; it attempts to answer the question but fails, causes confusion, and contradicts itself.\n",
    "- **Well**: The response is average; it answers the question but not completely, makes incorrect assumptions, and misrepresents concepts.\n",
    "- **Good**: The response is good; it answers the question completely but includes some errors.\n",
    "- **Excellent**: The response is excellent; it answers the question completely and correctly.\n",
    "\n",
    "You must also provide a **justification** for your score, explaining why you gave that rating and what aspects of the response led you to that conclusion.\n",
    "The main question is to give a explanation for a fraud detection system, so the response should be focused on that topic.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class JudgeScorer(BaseScorer):\n",
    "    def __init__(self, model:str, system_prompt:str=select_judge_system_prompt, user_prompt:str=user_prompt):\n",
    "        super().__init__(model=model, system_prompt=system_prompt, metric_name=\"correctness\", structure=JudgeOutput, user_prompt=user_prompt)\n",
    "        self.embeddings = None\n",
    "        self.scores_num = []\n",
    "        self.justifications = []\n",
    "\n",
    "    def process_scores(self):\n",
    "        numeric_scores = [score.score for score in self.scores]\n",
    "        justifications = [score.justification for score in self.scores]\n",
    "\n",
    "        self.embeddings = embedding_model.encode(justifications)\n",
    "\n",
    "        self.scores_num = numeric_scores\n",
    "        self.justifications = justifications\n",
    "\n",
    "\n",
    "    def make_embeddings_metrics(self):\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Debes ejecutar process_scores antes de calcular las métricas de embeddings.\")\n",
    "\n",
    "        distance_matrix = cosine_similarity(self.embeddings)\n",
    "\n",
    "        upper_tri_indices = np.triu_indices_from(distance_matrix, k=1)\n",
    "        upper_tri_values = distance_matrix[upper_tri_indices]\n",
    "\n",
    "        variance = np.var(upper_tri_values)\n",
    "        std_dev = np.std(upper_tri_values)\n",
    "\n",
    "        return {\n",
    "            \"cosine_similarity/mean\": np.mean(upper_tri_values),\n",
    "            \"cosine_similarity/std\": std_dev,\n",
    "            \"cosine_similarity/var\": variance\n",
    "        }\n",
    "\n",
    "    def make_similarity_metrics(self):\n",
    "\n",
    "        n = len(self.scores_num)\n",
    "\n",
    "        lev_matrix = np.zeros((n, n))\n",
    "        diff_matrix = np.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if  i<= j:\n",
    "                    lev_matrix[i, j] = levenshtein_ratio(self.justifications[i], self.justifications[j])\n",
    "                    diff_matrix[i, j] = SequenceMatcher(None, self.justifications[i], self.justifications[j]).ratio()\n",
    "\n",
    "        upper_tri_indices = np.triu_indices_from(lev_matrix, k=1)\n",
    "\n",
    "        upper_tri_lev = lev_matrix[upper_tri_indices]\n",
    "        upper_tri_diff = diff_matrix[upper_tri_indices]\n",
    "\n",
    "        return {\n",
    "            \"levenshtein_similarity/mean\": np.mean(upper_tri_lev),\n",
    "            \"levenshtein_similarity/std\": np.std(upper_tri_lev),\n",
    "            \"levenshtein_similarity/var\": np.var(upper_tri_lev),\n",
    "            \"diff_similarity/mean\": np.mean(upper_tri_diff),\n",
    "            \"diff_similarity/std\": np.std(upper_tri_diff),\n",
    "            \"diff_similarity/var\": np.var(upper_tri_diff)\n",
    "        }\n",
    "\n",
    "    def categorical_to_numeric(self, score: str) -> float:\n",
    "        score_mapping = {\n",
    "            \"Bad\": 0.0,\n",
    "            \"Regular\": 1.0,\n",
    "            \"Well\": 2.0,\n",
    "            \"Good\": 3.0,\n",
    "            \"Excellent\": 4.0\n",
    "        }\n",
    "\n",
    "        return score_mapping.get(score, np.nan)\n",
    "\n",
    "    def log_metrics(self):\n",
    "            mlflow.log_metrics(self.make_embeddings_metrics())\n",
    "            mlflow.log_metrics(self.make_similarity_metrics())\n",
    "\n",
    "            self.scores_num = [self.categorical_to_numeric(score) for score in self.scores_num if score is not None]\n",
    "\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"scores/mean\": np.mean(self.scores_num),\n",
    "                    \"scores/std\": np.std(self.scores_num),\n",
    "                    \"scores/var\": np.var(self.scores_num)\n",
    "                }\n",
    "            )"
   ],
   "id": "69eefb7b34309e39",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:08.440134Z",
     "start_time": "2025-08-09T22:37:08.130924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_name = \"judge_selection\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "id": "63fab9bf97721c28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///F:/Documentos/git/fraud_ethereum_explanability/mlruns/120965715118781918', creation_time=1754540211730, experiment_id='120965715118781918', last_update_time=1754540211730, lifecycle_stage='active', name='judge_selection', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:08.471998Z",
     "start_time": "2025-08-09T22:37:08.448686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts_df = pd.read_csv(\"data/prompts_example.csv\")\n",
    "example = prompts_df.iloc[0][\"prompt\"]\n",
    "\n",
    "with open(\"data/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "message  = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": example}\n",
    "]\n",
    "\n",
    "# test_response = ChatOllama(model=\"gpt-oss:20b\", temperature=0.0).invoke(message)"
   ],
   "id": "b55e3ca40e39ff50",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:08.487176Z",
     "start_time": "2025-08-09T22:37:08.478106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with open (\"data/test_response.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(test_response.content)"
   ],
   "id": "fc29f1558e63d12a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:37:08.501820Z",
     "start_time": "2025-08-09T22:37:08.493248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"data/test_response.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_response = f.read()"
   ],
   "id": "bb2741897d7f6cbb",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlflow.langchain.autolog()\n",
    "for model in tqdm(available_models):\n",
    "    run_name=f\"judge_{model}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        judge_scorer = JudgeScorer(model=model)\n",
    "\n",
    "        # Simulamos respuestas y preguntas\n",
    "        responses = [test_response] * 10\n",
    "        questions = [message[0][\"content\"] + \"\\n\\n\" + message[1][\"content\"]] * 10\n",
    "\n",
    "        scores = judge_scorer.score(responses, questions)\n",
    "        judge_scorer.process_scores()\n",
    "\n",
    "        judge_scorer.log_metrics()\n",
    "\n",
    "        print(f\"Modelo: {model} evaluado y métricas registradas.\")"
   ],
   "id": "e85b0b3f381033f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Caso especial como gpt",
   "id": "171cfb8a326ea635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:57:50.032697Z",
     "start_time": "2025-08-09T22:57:50.025697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"\"\"\n",
    "### Evaluation Criteria\n",
    "\n",
    "Rate the response according to the following criteria:\n",
    "\n",
    "- **Bad**: The response is very poor and does not answer the question.\n",
    "- **Regular**: The response is poor; it attempts to answer the question but fails, causes confusion, and contradicts itself.\n",
    "- **Well**: The response is average; it answers the question but not completely, makes incorrect assumptions, and misrepresents concepts.\n",
    "- **Good**: The response is good; it answers the question completely but includes some errors.\n",
    "- **Excellent**: The response is excellent; it answers the question completely and correctly.\n",
    "\n",
    "You must also provide a **justification** for your score, explaining why you gave that rating and what aspects of the response led you to that conclusion.\n",
    "The main question is to give a explanation for a fraud detection system, so the response should be focused on that topic.\n",
    "\n",
    "Your response **should** be in a JSON format with the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"score\": \"...\",\n",
    "    \"justification\": \"...\"\n",
    "}\n",
    "```\n",
    "\"\"\""
   ],
   "id": "d94e97368158375",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T22:58:59.273881Z",
     "start_time": "2025-08-09T22:57:53.092641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "model = \"gpt-oss:20b\"\n",
    "\n",
    "run_name=f\"judge_{model}\"\n",
    "\n",
    "def parse_json_response(response: str):\n",
    "    response = response.split(\"```json\")[1]\n",
    "\n",
    "    if not response:\n",
    "        return None\n",
    "    response = response.split(\"```\")[0].strip()\n",
    "\n",
    "    try:\n",
    "        response = json.loads(response)\n",
    "        res = JudgeOutput(**response)\n",
    "        return res\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "class JudgeGPT(JudgeScorer):\n",
    "    def __init__(self, model:str, system_prompt:str=select_judge_system_prompt, user_prompt:str=user_prompt):\n",
    "        super().__init__(model=model, system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "        self.structure = JudgeOutput\n",
    "\n",
    "    def score(self, responses: list, questions: list) -> list:\n",
    "        model = ChatOllama(model=self.model, temperature=0.0)\n",
    "\n",
    "        eval_responses = []\n",
    "\n",
    "        for response, question in zip(responses , questions):\n",
    "            message = [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"# Instructions:\\n\\n{self.user_prompt}\\n\\n {{'Answer to evaluate' : {response}}}\"}\n",
    "                ]\n",
    "\n",
    "            score = model.invoke(message)\n",
    "            score = parse_json_response(score.content)\n",
    "            eval_responses.append(score)\n",
    "\n",
    "        self.scores = eval_responses\n",
    "\n",
    "        return self.scores\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    judge_scorer = JudgeGPT(model=model)\n",
    "\n",
    "    # Simulamos respuestas y preguntas\n",
    "    responses = [test_response] * 10\n",
    "    questions = [message[0][\"content\"] + \"\\n\\n\" + message[1][\"content\"]] * 10\n",
    "\n",
    "    scores = judge_scorer.score(responses, questions)\n",
    "    judge_scorer.process_scores()\n",
    "\n",
    "    judge_scorer.log_metrics()\n",
    "\n",
    "    print(f\"Modelo: {model} evaluado y métricas registradas.\")\n"
   ],
   "id": "64a1fa71593f7e43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo: gpt-oss:20b\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "```json\n",
      "{\n",
      "    \"score\": \"Excellent\",\n",
      "    \"justification\": \"El texto responde de manera completa y precisa a la solicitud de explicar un sistema de detección de fraude en transacciones de Ethereum. Se presenta una estructura clara con secciones bien diferenciadas: variables transformadas, variables originales, importancia del modelo y análisis de la transacción específica. Cada tabla incluye valores, comentarios y una interpretación contextualizada, lo que facilita la comprensión del funcionamiento del modelo CatBoost. Además, se concluye con un resumen ejecutivo que resume los hallazgos clave y la decisión del modelo (FLAG = 1). No se observan contradicciones ni errores conceptuales significativos, y la información está bien organizada y escrita en Markdown, cumpliendo con los requisitos de formato. Por todo ello, la respuesta merece la calificación de \\\"Excellent\\\".\"\n",
      "}\n",
      "```\n",
      "Modelo: gpt-oss:20b evaluado y métricas registradas.\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
